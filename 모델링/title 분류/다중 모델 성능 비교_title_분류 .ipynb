{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9000d12",
   "metadata": {
    "id": "a9000d12"
   },
   "source": [
    "# CNN모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383f7da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in h:\\anaconda3\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in h:\\anaconda3\\lib\\site-packages (from konlpy) (4.9.1)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in h:\\anaconda3\\lib\\site-packages (from konlpy) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.6 in h:\\anaconda3\\lib\\site-packages (from konlpy) (1.21.5)\n",
      "Requirement already satisfied: packaging in h:\\anaconda3\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in h:\\anaconda3\\lib\\site-packages (from packaging->JPype1>=0.7.0->konlpy) (3.0.9)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28a6a893",
   "metadata": {
    "id": "28a6a893"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7c13c38",
   "metadata": {
    "id": "b7c13c38"
   },
   "outputs": [],
   "source": [
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(\"./라벨링완료된공지사항(수정본).csv\") \n",
    "\n",
    "notice_names = df['title'].tolist()\n",
    "\n",
    "# 전처리 함수 정의\n",
    "def preprocess_text(text):\n",
    "    # 특수문자 제거\n",
    "    text = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", text)\n",
    "\n",
    "    # 숫자 정보 제거\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # 형태소 분석\n",
    "    tagger = Okt()\n",
    "    words = tagger.morphs(text)\n",
    "\n",
    "    # 불용어 제거\n",
    "    stop_words = ['필독', '학기', '학년', '도', '년', '제', '회', '월', '학부', '일', '차', '년도', '안내']  # 불용어 리스트\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # 분석된 형태소들을 공백으로 결합하여 문장으로 반환\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "preprocessed_notice_names = [preprocess_text(notice_name) for notice_name in notice_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aae4507",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7aae4507",
    "outputId": "1ead9c15-266b-44b3-d4d9-eafe06611f70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 긴 문장의 길이: 29\n",
      "어휘 사전의 크기: 6097\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 생성\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# 토크나이저가 데이터를 학습\n",
    "tokenizer.fit_on_texts(preprocessed_notice_names)\n",
    "\n",
    "# 데이터를 시퀀스로 변환\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_notice_names)\n",
    "\n",
    "# 가장 긴 문장의 길이를 확인\n",
    "max_sequence_length = max(len(s) for s in sequences)\n",
    "print('가장 긴 문장의 길이:', max_sequence_length)\n",
    "\n",
    "# 어휘 사전의 크기를 확인 (+1을 하는 이유는 0인덱스를 고려하기 때문입니다.)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('어휘 사전의 크기:', vocab_size)\n",
    "\n",
    "# 모든 문장을 가장 긴 문장의 길이로 패딩 처리\n",
    "padded_X = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4262c19",
   "metadata": {
    "id": "c4262c19"
   },
   "outputs": [],
   "source": [
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_notice_names)\n",
    "\n",
    "tfidf_model = TfidfVectorizer().fit(preprocessed_notice_names)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bceda68d",
   "metadata": {
    "id": "bceda68d"
   },
   "outputs": [],
   "source": [
    "# 훈련 데이터 분리: train 데이터 80%, test 데이터 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "029f331b",
   "metadata": {
    "id": "029f331b"
   },
   "outputs": [],
   "source": [
    "# 타겟 변수 다중 클래스 원-핫 인코딩\n",
    "y_train_encoded = to_categorical(y_train)\n",
    "y_test_encoded = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f0cdfd7",
   "metadata": {
    "id": "5f0cdfd7"
   },
   "outputs": [],
   "source": [
    "# CNN 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128, input_length=max_sequence_length))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# 출력 뉴런의 개수와 활성화 함수를 클래스 개수에 따라 조정\n",
    "num_classes = len(set(y))  # 클래스 개수 계산\n",
    "if num_classes == 2:\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "else:\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e43c2fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8010, 29)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1c6ec4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[232, 355, 361, ...,   0,   0,   0],\n",
       "       [114, 185, 888, ...,   0,   0,   0],\n",
       "       [353, 325,  11, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [ 28,  37,  36, ...,   0,   0,   0],\n",
       "       [143, 543,  77, ...,   0,   0,   0],\n",
       "       [ 79, 121, 217, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4faa728e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35, 15, 14, ..., 37, 58, 33])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e9af3b2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e9af3b2",
    "outputId": "24b0eb97-1c0c-4f4f-ceaa-133825c81eb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "251/251 [==============================] - 4s 13ms/step - loss: 2.8208 - accuracy: 0.3659 - val_loss: 1.4425 - val_accuracy: 0.6710\n",
      "Epoch 2/10\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 0.6949 - accuracy: 0.8537 - val_loss: 0.5168 - val_accuracy: 0.8922\n",
      "Epoch 3/10\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 0.2015 - accuracy: 0.9598 - val_loss: 0.3844 - val_accuracy: 0.9066\n",
      "Epoch 4/10\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 0.0709 - accuracy: 0.9891 - val_loss: 0.3453 - val_accuracy: 0.9181\n",
      "Epoch 5/10\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 0.0290 - accuracy: 0.9960 - val_loss: 0.3375 - val_accuracy: 0.9246\n",
      "Epoch 6/10\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 0.0168 - accuracy: 0.9981 - val_loss: 0.3206 - val_accuracy: 0.9261\n",
      "Epoch 7/10\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 0.0095 - accuracy: 0.9990 - val_loss: 0.3322 - val_accuracy: 0.9236\n",
      "Epoch 8/10\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 0.0059 - accuracy: 0.9995 - val_loss: 0.3358 - val_accuracy: 0.9231\n",
      "Epoch 9/10\n",
      "251/251 [==============================] - 3s 12ms/step - loss: 0.0079 - accuracy: 0.9988 - val_loss: 0.3515 - val_accuracy: 0.9211\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ee8e9f8d90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모델 훈현\n",
    "model.fit(X_train, y_train_encoded, epochs=10, batch_size=32, validation_data=(X_test, y_test_encoded), callbacks=[EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09907760",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09907760",
    "outputId": "1c8c1e9e-187a-4ffc-bed8-f424065404b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9211183225162256\n"
     ]
    }
   ],
   "source": [
    "y_prob = model.predict(X_test, verbose=0) \n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "CNN_accuracy = np.sum(y_pred.flatten() == y_test) / len(y_test)\n",
    "print(\"Accuracy:\", CNN_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9qYC5C5pFUBl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9qYC5C5pFUBl",
    "outputId": "94c46090-1df9-43e9-f5f6-bd5cc086776c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "새로운 문장을 입력하세요: 서울아산병원 AMIST/울산의대 신약개발 사이버 썸머스쿨” 온라인 강의 개최\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "입력 문장: 서울아산병원 AMIST/울산의대 신약개발 사이버 썸머스쿨” 온라인 강의 개최\n",
      "예측된 레이블: 특강\n"
     ]
    }
   ],
   "source": [
    "# 새로운 입력 데이터 예측\n",
    "new_text = input(\"새로운 문장을 입력하세요: \")\n",
    "preprocessed_new_text = preprocess_text(new_text)\n",
    "sequence = tokenizer.texts_to_sequences([preprocessed_new_text])\n",
    "padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length, padding='post')\n",
    "prediction = model.predict(padded_sequence)\n",
    "predicted_label = label_encoder.inverse_transform([prediction.argmax()])[0]\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(f\"입력 문장: {new_text}\")\n",
    "print(f\"예측된 레이블: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1d295a",
   "metadata": {
    "id": "df1d295a"
   },
   "source": [
    "# MLP모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a648137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "52df0f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(\"./라벨링완료된공지사항(수정본).csv\") \n",
    "\n",
    "notice_names = df['title'].tolist()\n",
    "\n",
    "# 전처리 함수 정의\n",
    "def preprocess_text(text):\n",
    "    # 특수문자 제거\n",
    "    text = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", text)\n",
    "\n",
    "    # 숫자 정보 제거\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # 형태소 분석\n",
    "    tagger = Okt()\n",
    "    words = tagger.morphs(text)\n",
    "\n",
    "    # 불용어 제거\n",
    "    stop_words = ['필독', '학기', '학년', '도', '년', '제', '회', '월', '학부', '일', '차', '년도', '안내']  # 불용어 리스트\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # 분석된 형태소들을 공백으로 결합하여 문장으로 반환\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "preprocessed_notice_names = [preprocess_text(notice_name) for notice_name in notice_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "5cf92a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 긴 문장의 길이: 29\n",
      "어휘 사전의 크기: 6097\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 생성\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# 토크나이저가 데이터를 학습\n",
    "tokenizer.fit_on_texts(preprocessed_notice_names)\n",
    "\n",
    "# 데이터를 시퀀스로 변환\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_notice_names)\n",
    "\n",
    "# 가장 긴 문장의 길이를 확인\n",
    "max_sequence_length = max(len(s) for s in sequences)\n",
    "print('가장 긴 문장의 길이:', max_sequence_length)\n",
    "\n",
    "# 어휘 사전의 크기를 확인 (+1을 하는 이유는 0인덱스를 고려하기 때문입니다.)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('어휘 사전의 크기:', vocab_size)\n",
    "\n",
    "# 모든 문장을 가장 긴 문장의 길이로 패딩 처리\n",
    "padded_X = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "92bdbec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_notice_names)\n",
    "\n",
    "tfidf_model = TfidfVectorizer().fit(preprocessed_notice_names)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "900effb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_X, y[:len(padded_X)], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "308a0fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(max_sequence_length,)))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(len(label_encoder.classes_), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "868f893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "5cf0f4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[232, 355, 361, ...,   0,   0,   0],\n",
       "       [114, 185, 888, ...,   0,   0,   0],\n",
       "       [353, 325,  11, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [ 28,  37,  36, ...,   0,   0,   0],\n",
       "       [143, 543,  77, ...,   0,   0,   0],\n",
       "       [ 79, 121, 217, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "76992193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35, 15, 14, ..., 37, 58, 33])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "a7dade36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "251/251 [==============================] - 0s 1ms/step - loss: 40.9803 - accuracy: 0.0458 - val_loss: 4.9570 - val_accuracy: 0.0854\n",
      "Epoch 2/100\n",
      "251/251 [==============================] - 0s 796us/step - loss: 4.3616 - accuracy: 0.0839 - val_loss: 4.2588 - val_accuracy: 0.0854\n",
      "Epoch 3/100\n",
      "251/251 [==============================] - 0s 818us/step - loss: 4.0708 - accuracy: 0.0846 - val_loss: 4.0762 - val_accuracy: 0.0869\n",
      "Epoch 4/100\n",
      "251/251 [==============================] - 0s 834us/step - loss: 3.9662 - accuracy: 0.0850 - val_loss: 3.9847 - val_accuracy: 0.0869\n",
      "Epoch 5/100\n",
      "251/251 [==============================] - 0s 823us/step - loss: 3.9110 - accuracy: 0.0851 - val_loss: 3.9196 - val_accuracy: 0.0874\n",
      "Epoch 6/100\n",
      "251/251 [==============================] - 0s 769us/step - loss: 3.8765 - accuracy: 0.0854 - val_loss: 3.8846 - val_accuracy: 0.0874\n",
      "Epoch 7/100\n",
      "251/251 [==============================] - 0s 773us/step - loss: 3.8558 - accuracy: 0.0853 - val_loss: 3.8622 - val_accuracy: 0.0874\n",
      "Epoch 8/100\n",
      "251/251 [==============================] - 0s 801us/step - loss: 3.8251 - accuracy: 0.0858 - val_loss: 3.8437 - val_accuracy: 0.0874\n",
      "Epoch 9/100\n",
      "251/251 [==============================] - 0s 780us/step - loss: 3.8105 - accuracy: 0.0858 - val_loss: 3.8315 - val_accuracy: 0.0874\n",
      "Epoch 10/100\n",
      "251/251 [==============================] - 0s 783us/step - loss: 3.7992 - accuracy: 0.0859 - val_loss: 3.8232 - val_accuracy: 0.0874\n",
      "Epoch 11/100\n",
      "251/251 [==============================] - 0s 785us/step - loss: 3.7917 - accuracy: 0.0860 - val_loss: 3.8161 - val_accuracy: 0.0874\n",
      "Epoch 12/100\n",
      "251/251 [==============================] - 0s 779us/step - loss: 3.7862 - accuracy: 0.0860 - val_loss: 3.8122 - val_accuracy: 0.0874\n",
      "Epoch 13/100\n",
      "251/251 [==============================] - 0s 780us/step - loss: 3.7823 - accuracy: 0.0860 - val_loss: 3.8093 - val_accuracy: 0.0874\n",
      "Epoch 14/100\n",
      "251/251 [==============================] - 0s 771us/step - loss: 3.7793 - accuracy: 0.0859 - val_loss: 3.8075 - val_accuracy: 0.0874\n",
      "Epoch 15/100\n",
      "251/251 [==============================] - 0s 780us/step - loss: 3.7770 - accuracy: 0.0863 - val_loss: 3.8056 - val_accuracy: 0.0874\n",
      "Epoch 16/100\n",
      "251/251 [==============================] - 0s 770us/step - loss: 3.7754 - accuracy: 0.0860 - val_loss: 3.8086 - val_accuracy: 0.0874\n",
      "Epoch 17/100\n",
      "251/251 [==============================] - 0s 778us/step - loss: 3.7751 - accuracy: 0.0864 - val_loss: 3.7814 - val_accuracy: 0.0879\n",
      "Epoch 18/100\n",
      "251/251 [==============================] - 0s 774us/step - loss: 3.7730 - accuracy: 0.0863 - val_loss: 3.7837 - val_accuracy: 0.0879\n",
      "Epoch 19/100\n",
      "251/251 [==============================] - 0s 788us/step - loss: 3.7718 - accuracy: 0.0864 - val_loss: 3.7828 - val_accuracy: 0.0879\n",
      "Epoch 20/100\n",
      "251/251 [==============================] - 0s 783us/step - loss: 3.7714 - accuracy: 0.0863 - val_loss: 3.7827 - val_accuracy: 0.0879\n",
      "Epoch 21/100\n",
      "251/251 [==============================] - 0s 807us/step - loss: 3.7710 - accuracy: 0.0861 - val_loss: 3.7836 - val_accuracy: 0.0879\n",
      "Epoch 22/100\n",
      "251/251 [==============================] - 0s 816us/step - loss: 3.7723 - accuracy: 0.0861 - val_loss: 3.7803 - val_accuracy: 0.0874\n",
      "Epoch 23/100\n",
      "251/251 [==============================] - 0s 775us/step - loss: 3.7707 - accuracy: 0.0861 - val_loss: 3.7794 - val_accuracy: 0.0874\n",
      "Epoch 24/100\n",
      "251/251 [==============================] - 0s 823us/step - loss: 3.7701 - accuracy: 0.0861 - val_loss: 3.7828 - val_accuracy: 0.0864\n",
      "Epoch 25/100\n",
      "251/251 [==============================] - 0s 832us/step - loss: 3.7704 - accuracy: 0.0864 - val_loss: 3.7830 - val_accuracy: 0.0869\n",
      "Epoch 26/100\n",
      "251/251 [==============================] - 0s 804us/step - loss: 3.7723 - accuracy: 0.0860 - val_loss: 3.7768 - val_accuracy: 0.0874\n",
      "Epoch 27/100\n",
      "251/251 [==============================] - 0s 791us/step - loss: 3.7731 - accuracy: 0.0860 - val_loss: 3.7756 - val_accuracy: 0.0874\n",
      "Epoch 28/100\n",
      "251/251 [==============================] - 0s 775us/step - loss: 3.7710 - accuracy: 0.0859 - val_loss: 3.7755 - val_accuracy: 0.0874\n",
      "Epoch 29/100\n",
      "251/251 [==============================] - 0s 777us/step - loss: 3.7709 - accuracy: 0.0859 - val_loss: 3.7755 - val_accuracy: 0.0874\n",
      "Epoch 30/100\n",
      "251/251 [==============================] - 0s 777us/step - loss: 3.7708 - accuracy: 0.0859 - val_loss: 3.7755 - val_accuracy: 0.0874\n",
      "Epoch 31/100\n",
      "251/251 [==============================] - 0s 766us/step - loss: 3.7707 - accuracy: 0.0859 - val_loss: 3.7756 - val_accuracy: 0.0874\n",
      "Epoch 32/100\n",
      "251/251 [==============================] - 0s 786us/step - loss: 3.7706 - accuracy: 0.0859 - val_loss: 3.7757 - val_accuracy: 0.0874\n",
      "Epoch 33/100\n",
      "251/251 [==============================] - 0s 779us/step - loss: 3.7705 - accuracy: 0.0859 - val_loss: 3.7757 - val_accuracy: 0.0874\n",
      "Epoch 34/100\n",
      "251/251 [==============================] - 0s 774us/step - loss: 3.7705 - accuracy: 0.0859 - val_loss: 3.7757 - val_accuracy: 0.0874\n",
      "Epoch 35/100\n",
      "251/251 [==============================] - 0s 825us/step - loss: 3.7705 - accuracy: 0.0859 - val_loss: 3.7758 - val_accuracy: 0.0874\n",
      "Epoch 36/100\n",
      "251/251 [==============================] - 0s 793us/step - loss: 3.7704 - accuracy: 0.0859 - val_loss: 3.7757 - val_accuracy: 0.0874\n",
      "Epoch 37/100\n",
      "251/251 [==============================] - 0s 777us/step - loss: 3.7704 - accuracy: 0.0859 - val_loss: 3.7758 - val_accuracy: 0.0874\n",
      "Epoch 38/100\n",
      "251/251 [==============================] - 0s 773us/step - loss: 3.7704 - accuracy: 0.0859 - val_loss: 3.7759 - val_accuracy: 0.0874\n",
      "Epoch 39/100\n",
      "251/251 [==============================] - 0s 774us/step - loss: 3.7704 - accuracy: 0.0859 - val_loss: 3.7760 - val_accuracy: 0.0874\n",
      "Epoch 40/100\n",
      "251/251 [==============================] - 0s 781us/step - loss: 3.7703 - accuracy: 0.0859 - val_loss: 3.7761 - val_accuracy: 0.0874\n",
      "Epoch 41/100\n",
      "251/251 [==============================] - 0s 834us/step - loss: 3.7703 - accuracy: 0.0859 - val_loss: 3.7761 - val_accuracy: 0.0874\n",
      "Epoch 42/100\n",
      "251/251 [==============================] - 0s 783us/step - loss: 3.7703 - accuracy: 0.0859 - val_loss: 3.7761 - val_accuracy: 0.0874\n",
      "Epoch 43/100\n",
      "251/251 [==============================] - 0s 777us/step - loss: 3.7703 - accuracy: 0.0859 - val_loss: 3.7762 - val_accuracy: 0.0874\n",
      "Epoch 44/100\n",
      "251/251 [==============================] - 0s 779us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7763 - val_accuracy: 0.0874\n",
      "Epoch 45/100\n",
      "251/251 [==============================] - 0s 787us/step - loss: 3.7703 - accuracy: 0.0859 - val_loss: 3.7763 - val_accuracy: 0.0874\n",
      "Epoch 46/100\n",
      "251/251 [==============================] - 0s 792us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7764 - val_accuracy: 0.0874\n",
      "Epoch 47/100\n",
      "251/251 [==============================] - 0s 820us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7765 - val_accuracy: 0.0874\n",
      "Epoch 48/100\n",
      "251/251 [==============================] - 0s 782us/step - loss: 3.7703 - accuracy: 0.0859 - val_loss: 3.7766 - val_accuracy: 0.0874\n",
      "Epoch 49/100\n",
      "251/251 [==============================] - 0s 770us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7767 - val_accuracy: 0.0874\n",
      "Epoch 50/100\n",
      "251/251 [==============================] - 0s 773us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7766 - val_accuracy: 0.0874\n",
      "Epoch 51/100\n",
      "251/251 [==============================] - 0s 773us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7768 - val_accuracy: 0.0874\n",
      "Epoch 52/100\n",
      "251/251 [==============================] - 0s 789us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7768 - val_accuracy: 0.0874\n",
      "Epoch 53/100\n",
      "251/251 [==============================] - 0s 778us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7769 - val_accuracy: 0.0874\n",
      "Epoch 54/100\n",
      "251/251 [==============================] - 0s 768us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7769 - val_accuracy: 0.0874\n",
      "Epoch 55/100\n",
      "251/251 [==============================] - 0s 774us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7769 - val_accuracy: 0.0874\n",
      "Epoch 56/100\n",
      "251/251 [==============================] - 0s 783us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7770 - val_accuracy: 0.0874\n",
      "Epoch 57/100\n",
      "251/251 [==============================] - 0s 812us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7770 - val_accuracy: 0.0874\n",
      "Epoch 58/100\n",
      "251/251 [==============================] - 0s 804us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7771 - val_accuracy: 0.0874\n",
      "Epoch 59/100\n",
      "251/251 [==============================] - 0s 812us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7771 - val_accuracy: 0.0874\n",
      "Epoch 60/100\n",
      "251/251 [==============================] - 0s 834us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7771 - val_accuracy: 0.0874\n",
      "Epoch 61/100\n",
      "251/251 [==============================] - 0s 843us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7772 - val_accuracy: 0.0874\n",
      "Epoch 62/100\n",
      "251/251 [==============================] - 0s 840us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7772 - val_accuracy: 0.0874\n",
      "Epoch 63/100\n",
      "251/251 [==============================] - 0s 864us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7772 - val_accuracy: 0.0874\n",
      "Epoch 64/100\n",
      "251/251 [==============================] - 0s 820us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7773 - val_accuracy: 0.0874\n",
      "Epoch 65/100\n",
      "251/251 [==============================] - 0s 792us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7773 - val_accuracy: 0.0874\n",
      "Epoch 66/100\n",
      "251/251 [==============================] - 0s 812us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7774 - val_accuracy: 0.0874\n",
      "Epoch 67/100\n",
      "251/251 [==============================] - 0s 807us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7775 - val_accuracy: 0.0874\n",
      "Epoch 68/100\n",
      "251/251 [==============================] - 0s 787us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7774 - val_accuracy: 0.0874\n",
      "Epoch 69/100\n",
      "251/251 [==============================] - 0s 772us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7776 - val_accuracy: 0.0874\n",
      "Epoch 70/100\n",
      "251/251 [==============================] - 0s 766us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7776 - val_accuracy: 0.0874\n",
      "Epoch 71/100\n",
      "251/251 [==============================] - 0s 805us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7776 - val_accuracy: 0.0874\n",
      "Epoch 72/100\n",
      "251/251 [==============================] - 0s 808us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7776 - val_accuracy: 0.0874\n",
      "Epoch 73/100\n",
      "251/251 [==============================] - 0s 780us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7777 - val_accuracy: 0.0874\n",
      "Epoch 74/100\n",
      "251/251 [==============================] - 0s 781us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7777 - val_accuracy: 0.0874\n",
      "Epoch 75/100\n",
      "251/251 [==============================] - 0s 778us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7777 - val_accuracy: 0.0874\n",
      "Epoch 76/100\n",
      "251/251 [==============================] - 0s 786us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7778 - val_accuracy: 0.0874\n",
      "Epoch 77/100\n",
      "251/251 [==============================] - 0s 776us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7778 - val_accuracy: 0.0874\n",
      "Epoch 78/100\n",
      "251/251 [==============================] - 0s 776us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7779 - val_accuracy: 0.0874\n",
      "Epoch 79/100\n",
      "251/251 [==============================] - 0s 773us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7779 - val_accuracy: 0.0874\n",
      "Epoch 80/100\n",
      "251/251 [==============================] - 0s 775us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7780 - val_accuracy: 0.0874\n",
      "Epoch 81/100\n",
      "251/251 [==============================] - 0s 783us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7780 - val_accuracy: 0.0874\n",
      "Epoch 82/100\n",
      "251/251 [==============================] - 0s 786us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7781 - val_accuracy: 0.0874\n",
      "Epoch 83/100\n",
      "251/251 [==============================] - 0s 787us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7781 - val_accuracy: 0.0874\n",
      "Epoch 84/100\n",
      "251/251 [==============================] - 0s 775us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7782 - val_accuracy: 0.0874\n",
      "Epoch 85/100\n",
      "251/251 [==============================] - 0s 774us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7782 - val_accuracy: 0.0874\n",
      "Epoch 86/100\n",
      "251/251 [==============================] - 0s 766us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7781 - val_accuracy: 0.0874\n",
      "Epoch 87/100\n",
      "251/251 [==============================] - 0s 791us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7782 - val_accuracy: 0.0874\n",
      "Epoch 88/100\n",
      "251/251 [==============================] - 0s 777us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7783 - val_accuracy: 0.0874\n",
      "Epoch 89/100\n",
      "251/251 [==============================] - 0s 779us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7783 - val_accuracy: 0.0874\n",
      "Epoch 90/100\n",
      "251/251 [==============================] - 0s 778us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7783 - val_accuracy: 0.0874\n",
      "Epoch 91/100\n",
      "251/251 [==============================] - 0s 787us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7784 - val_accuracy: 0.0874\n",
      "Epoch 92/100\n",
      "251/251 [==============================] - 0s 781us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7783 - val_accuracy: 0.0874\n",
      "Epoch 93/100\n",
      "251/251 [==============================] - 0s 778us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7783 - val_accuracy: 0.0874\n",
      "Epoch 94/100\n",
      "251/251 [==============================] - 0s 768us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7783 - val_accuracy: 0.0874\n",
      "Epoch 95/100\n",
      "251/251 [==============================] - 0s 769us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7783 - val_accuracy: 0.0874\n",
      "Epoch 96/100\n",
      "251/251 [==============================] - 0s 773us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7784 - val_accuracy: 0.0874\n",
      "Epoch 97/100\n",
      "251/251 [==============================] - 0s 778us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7784 - val_accuracy: 0.0874\n",
      "Epoch 98/100\n",
      "251/251 [==============================] - 0s 775us/step - loss: 3.7701 - accuracy: 0.0859 - val_loss: 3.7785 - val_accuracy: 0.0874\n",
      "Epoch 99/100\n",
      "251/251 [==============================] - 0s 775us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7785 - val_accuracy: 0.0874\n",
      "Epoch 100/100\n",
      "251/251 [==============================] - 0s 777us/step - loss: 3.7702 - accuracy: 0.0859 - val_loss: 3.7784 - val_accuracy: 0.0874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28fada28910>"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "1995a7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.08736894658012981\n"
     ]
    }
   ],
   "source": [
    "y_prob = model.predict(X_test, verbose=0) \n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "MLP_accuracy = np.sum(y_pred.flatten() == y_test) / len(y_test)\n",
    "print(\"Accuracy:\", MLP_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b265924",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3fbc0131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "3610c90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(\"./라벨링완료된공지사항(수정본).csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "4fbf05fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 전처리 테스트가 벡터로 변환\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['title'])\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "aee79db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#훈련 데이터 분리 train 데이터 80%, test 데이터 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "91bb691f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SVC()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6cd1be2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7903145282076884\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "SVM_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"정확도:\", SVM_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f803d8a5",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "415fe8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "134bcc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(\"./라벨링완료된공지사항(수정본).csv\") \n",
    "\n",
    "notice_names = df['title'].tolist()\n",
    "\n",
    "# 전처리 함수 정의\n",
    "def preprocess_text(text):\n",
    "    # 특수문자 제거\n",
    "    text = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", text)\n",
    "\n",
    "    # 숫자 정보 제거\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # 형태소 분석\n",
    "    tagger = Okt()\n",
    "    words = tagger.morphs(text)\n",
    "\n",
    "    # 불용어 제거\n",
    "    stop_words = ['필독', '학기', '학년', '도', '년', '제', '회', '월', '학부', '일', '차', '년도', '안내']  # 불용어 리스트\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # 분석된 형태소들을 공백으로 결합하여 문장으로 반환\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "preprocessed_notice_names = [preprocess_text(notice_name) for notice_name in notice_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "93035491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_notice_names)\n",
    "\n",
    "tfidf_model = TfidfVectorizer().fit(preprocessed_notice_names)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "53f9ded6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 긴 문장의 길이: 29\n",
      "어휘 사전의 크기: 6097\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 생성\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# 토크나이저가 데이터를 학습\n",
    "tokenizer.fit_on_texts(preprocessed_notice_names)\n",
    "\n",
    "# 데이터를 시퀀스로 변환\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_notice_names)\n",
    "\n",
    "# 가장 긴 문장의 길이를 확인\n",
    "max_sequence_length = max(len(s) for s in sequences)\n",
    "print('가장 긴 문장의 길이:', max_sequence_length)\n",
    "\n",
    "# 어휘 사전의 크기를 확인 (+1을 하는 이유는 0인덱스를 고려하기 때문입니다.)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('어휘 사전의 크기:', vocab_size)\n",
    "\n",
    "# 모든 문장을 가장 긴 문장의 길이로 패딩 처리\n",
    "padded_X = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1f043143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_notice_names)\n",
    "\n",
    "tfidf_model = TfidfVectorizer().fit(preprocessed_notice_names)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6c921499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 분리: train 데이터 80%, test 데이터 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6683fe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타겟 변수 다중 클래스 원-핫 인코딩\n",
    "y_train_encoded = to_categorical(y_train)\n",
    "y_test_encoded = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d44c1571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128, input_length=max_sequence_length))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# 출력 뉴런의 개수와 활성화 함수를 클래스 개수에 따라 조정\n",
    "num_classes = len(set(y))  # 클래스 개수 계산\n",
    "if num_classes == 2:\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "else:\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b448d8ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "251/251 [==============================] - 4s 11ms/step - loss: 3.7911 - accuracy: 0.0793 - val_loss: 3.5480 - val_accuracy: 0.1048\n",
      "Epoch 2/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 3.3451 - accuracy: 0.1146 - val_loss: 3.0594 - val_accuracy: 0.1578\n",
      "Epoch 3/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 2.7736 - accuracy: 0.2047 - val_loss: 2.6753 - val_accuracy: 0.2267\n",
      "Epoch 4/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 2.3126 - accuracy: 0.3079 - val_loss: 2.2554 - val_accuracy: 0.3615\n",
      "Epoch 5/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 1.7378 - accuracy: 0.4866 - val_loss: 1.8202 - val_accuracy: 0.5162\n",
      "Epoch 6/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 1.2747 - accuracy: 0.6213 - val_loss: 1.6335 - val_accuracy: 0.5736\n",
      "Epoch 7/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.9366 - accuracy: 0.7199 - val_loss: 1.4283 - val_accuracy: 0.6555\n",
      "Epoch 8/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.6716 - accuracy: 0.8047 - val_loss: 1.2428 - val_accuracy: 0.7369\n",
      "Epoch 9/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.4593 - accuracy: 0.8821 - val_loss: 1.1490 - val_accuracy: 0.7818\n",
      "Epoch 10/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.3300 - accuracy: 0.9171 - val_loss: 1.0697 - val_accuracy: 0.7958\n",
      "Epoch 11/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.2162 - accuracy: 0.9466 - val_loss: 1.0295 - val_accuracy: 0.8183\n",
      "Epoch 12/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.1574 - accuracy: 0.9634 - val_loss: 1.0360 - val_accuracy: 0.8283\n",
      "Epoch 13/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.1263 - accuracy: 0.9712 - val_loss: 1.0694 - val_accuracy: 0.8288\n",
      "Epoch 14/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.1066 - accuracy: 0.9760 - val_loss: 1.0787 - val_accuracy: 0.8382\n",
      "Epoch 15/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0823 - accuracy: 0.9828 - val_loss: 1.0747 - val_accuracy: 0.8417\n",
      "Epoch 16/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0878 - accuracy: 0.9783 - val_loss: 1.0683 - val_accuracy: 0.8387\n",
      "Epoch 17/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0431 - accuracy: 0.9898 - val_loss: 1.0867 - val_accuracy: 0.8517\n",
      "Epoch 18/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0379 - accuracy: 0.9905 - val_loss: 1.1273 - val_accuracy: 0.8442\n",
      "Epoch 19/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0582 - accuracy: 0.9843 - val_loss: 1.0800 - val_accuracy: 0.8507\n",
      "Epoch 20/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0367 - accuracy: 0.9919 - val_loss: 1.1099 - val_accuracy: 0.8522\n",
      "Epoch 21/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0449 - accuracy: 0.9888 - val_loss: 1.1874 - val_accuracy: 0.8323\n",
      "Epoch 22/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.0545 - accuracy: 0.9865 - val_loss: 1.1937 - val_accuracy: 0.8407\n",
      "Epoch 23/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0355 - accuracy: 0.9921 - val_loss: 1.1523 - val_accuracy: 0.8577\n",
      "Epoch 24/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0175 - accuracy: 0.9960 - val_loss: 1.1668 - val_accuracy: 0.8532\n",
      "Epoch 25/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0138 - accuracy: 0.9963 - val_loss: 1.1996 - val_accuracy: 0.8512\n",
      "Epoch 26/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0570 - accuracy: 0.9858 - val_loss: 1.1854 - val_accuracy: 0.8517\n",
      "Epoch 27/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0284 - accuracy: 0.9939 - val_loss: 1.2093 - val_accuracy: 0.8512\n",
      "Epoch 28/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0223 - accuracy: 0.9943 - val_loss: 1.1459 - val_accuracy: 0.8552\n",
      "Epoch 29/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0150 - accuracy: 0.9966 - val_loss: 1.2322 - val_accuracy: 0.8557\n",
      "Epoch 30/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0208 - accuracy: 0.9945 - val_loss: 1.1609 - val_accuracy: 0.8597\n",
      "Epoch 31/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.0065 - accuracy: 0.9983 - val_loss: 1.2129 - val_accuracy: 0.8587\n",
      "Epoch 32/50\n",
      "251/251 [==============================] - 3s 11ms/step - loss: 0.0041 - accuracy: 0.9990 - val_loss: 1.1976 - val_accuracy: 0.8582\n",
      "Epoch 33/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 1.2273 - val_accuracy: 0.8572\n",
      "Epoch 34/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.0194 - accuracy: 0.9958 - val_loss: 1.4818 - val_accuracy: 0.8278\n",
      "Epoch 35/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.0397 - accuracy: 0.9895 - val_loss: 1.2657 - val_accuracy: 0.8432\n",
      "Epoch 36/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.0351 - accuracy: 0.9909 - val_loss: 1.2384 - val_accuracy: 0.8452\n",
      "Epoch 37/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0153 - accuracy: 0.9951 - val_loss: 1.1889 - val_accuracy: 0.8552\n",
      "Epoch 38/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0143 - accuracy: 0.9965 - val_loss: 1.1828 - val_accuracy: 0.8572\n",
      "Epoch 39/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0090 - accuracy: 0.9983 - val_loss: 1.2384 - val_accuracy: 0.8542\n",
      "Epoch 40/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.0085 - accuracy: 0.9983 - val_loss: 1.1953 - val_accuracy: 0.8607\n",
      "Epoch 41/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 1.2117 - val_accuracy: 0.8617\n",
      "Epoch 42/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.0031 - accuracy: 0.9989 - val_loss: 1.2339 - val_accuracy: 0.8627\n",
      "Epoch 43/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.0023 - accuracy: 0.9991 - val_loss: 1.2485 - val_accuracy: 0.8627\n",
      "Epoch 44/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.0025 - accuracy: 0.9990 - val_loss: 1.2616 - val_accuracy: 0.8647\n",
      "Epoch 45/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.0024 - accuracy: 0.9990 - val_loss: 1.2822 - val_accuracy: 0.8587\n",
      "Epoch 46/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 1.2834 - val_accuracy: 0.8582\n",
      "Epoch 47/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 1.3210 - val_accuracy: 0.8592\n",
      "Epoch 48/50\n",
      "251/251 [==============================] - 3s 10ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 1.3376 - val_accuracy: 0.8627\n",
      "Epoch 49/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 1.3430 - val_accuracy: 0.8637\n",
      "Epoch 50/50\n",
      "251/251 [==============================] - 2s 10ms/step - loss: 0.0028 - accuracy: 0.9989 - val_loss: 1.3436 - val_accuracy: 0.8607\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28f90739580>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "model.fit(X_train, y_train_encoded, epochs=50, batch_size=32, validation_data=(X_test, y_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ecf66efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8607089365951074\n"
     ]
    }
   ],
   "source": [
    "y_prob = model.predict(X_test, verbose=0) \n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "LSTM_accuracy = np.sum(y_pred.flatten() == y_test) / len(y_test)\n",
    "print(\"Accuracy:\", LSTM_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f3b305",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c11569bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "35e848f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(\"./라벨링완료된공지사항(수정본).csv\") \n",
    "\n",
    "notice_names = df['title'].tolist()\n",
    "\n",
    "# 전처리 함수 정의\n",
    "def preprocess_text(text):\n",
    "    # 특수문자 제거\n",
    "    text = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", text)\n",
    "\n",
    "    # 숫자 정보 제거\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # 형태소 분석\n",
    "    tagger = Okt()\n",
    "    words = tagger.morphs(text)\n",
    "\n",
    "    # 불용어 제거\n",
    "    stop_words = ['필독', '학기', '학년', '도', '년', '제', '회', '월', '학부', '일', '차', '년도', '안내']  # 불용어 리스트\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # 분석된 형태소들을 공백으로 결합하여 문장으로 반환\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "preprocessed_notice_names = [preprocess_text(notice_name) for notice_name in notice_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5bb3f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_notice_names)\n",
    "\n",
    "tfidf_model = TfidfVectorizer().fit(preprocessed_notice_names)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "42e393d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 긴 문장의 길이: 29\n",
      "어휘 사전의 크기: 6097\n"
     ]
    }
   ],
   "source": [
    "# 토크나이저 생성\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# 토크나이저가 데이터를 학습\n",
    "tokenizer.fit_on_texts(preprocessed_notice_names)\n",
    "\n",
    "# 데이터를 시퀀스로 변환\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_notice_names)\n",
    "\n",
    "# 가장 긴 문장의 길이를 확인\n",
    "max_sequence_length = max(len(s) for s in sequences)\n",
    "print('가장 긴 문장의 길이:', max_sequence_length)\n",
    "\n",
    "# 어휘 사전의 크기를 확인 (+1을 하는 이유는 0인덱스를 고려하기 때문입니다.)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('어휘 사전의 크기:', vocab_size)\n",
    "\n",
    "# 모든 문장을 가장 긴 문장의 길이로 패딩 처리\n",
    "padded_X = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d1a586a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 분리: train 데이터 80%, test 데이터 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "593d2d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타겟 변수 다중 클래스 원-핫 인코딩\n",
    "y_train_encoded = to_categorical(y_train)\n",
    "y_test_encoded = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "9c3bd594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUR모델 생성\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128, input_length=max_sequence_length))\n",
    "model.add(GRU(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation='softmax'))\n",
    "\n",
    "# 출력 뉴런의 개수와 활성화 함수를 클래스 개수에 따라 조정\n",
    "num_classes = len(set(y))  # 클래스 개수 계산\n",
    "if num_classes == 2:\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "else:\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "9c4a673b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 1.0689 - accuracy: 0.6426 - val_loss: 1.9011 - val_accuracy: 0.5657\n",
      "Epoch 2/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 1.0553 - accuracy: 0.6429 - val_loss: 1.8917 - val_accuracy: 0.5652\n",
      "Epoch 3/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 1.0592 - accuracy: 0.6412 - val_loss: 1.8947 - val_accuracy: 0.5796\n",
      "Epoch 4/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 1.0537 - accuracy: 0.6508 - val_loss: 1.9027 - val_accuracy: 0.5761\n",
      "Epoch 5/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 1.0456 - accuracy: 0.6499 - val_loss: 1.8858 - val_accuracy: 0.5806\n",
      "Epoch 6/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 1.0348 - accuracy: 0.6512 - val_loss: 1.8520 - val_accuracy: 0.5831\n",
      "Epoch 7/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 1.0191 - accuracy: 0.6654 - val_loss: 1.8770 - val_accuracy: 0.5886\n",
      "Epoch 8/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 1.0107 - accuracy: 0.6687 - val_loss: 1.8894 - val_accuracy: 0.6011\n",
      "Epoch 9/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 1.0035 - accuracy: 0.6668 - val_loss: 1.8888 - val_accuracy: 0.6031\n",
      "Epoch 10/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.9929 - accuracy: 0.6768 - val_loss: 1.8995 - val_accuracy: 0.6001\n",
      "Epoch 11/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.9830 - accuracy: 0.6743 - val_loss: 1.9148 - val_accuracy: 0.5931\n",
      "Epoch 12/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.9772 - accuracy: 0.6789 - val_loss: 1.8658 - val_accuracy: 0.5926\n",
      "Epoch 13/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.9707 - accuracy: 0.6747 - val_loss: 1.8931 - val_accuracy: 0.6081\n",
      "Epoch 14/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.9623 - accuracy: 0.6835 - val_loss: 1.8276 - val_accuracy: 0.5961\n",
      "Epoch 15/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.9601 - accuracy: 0.6775 - val_loss: 1.8572 - val_accuracy: 0.5951\n",
      "Epoch 16/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.9516 - accuracy: 0.6784 - val_loss: 1.8539 - val_accuracy: 0.5961\n",
      "Epoch 17/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.9442 - accuracy: 0.6828 - val_loss: 1.8794 - val_accuracy: 0.6011\n",
      "Epoch 18/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.9348 - accuracy: 0.6830 - val_loss: 1.8981 - val_accuracy: 0.6006\n",
      "Epoch 19/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.9325 - accuracy: 0.6818 - val_loss: 1.8828 - val_accuracy: 0.6061\n",
      "Epoch 20/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.9259 - accuracy: 0.6868 - val_loss: 1.8524 - val_accuracy: 0.6016\n",
      "Epoch 21/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.9179 - accuracy: 0.6919 - val_loss: 1.8351 - val_accuracy: 0.6246\n",
      "Epoch 22/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.9076 - accuracy: 0.7031 - val_loss: 1.8529 - val_accuracy: 0.6241\n",
      "Epoch 23/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.9046 - accuracy: 0.7000 - val_loss: 1.8514 - val_accuracy: 0.6066\n",
      "Epoch 24/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.9017 - accuracy: 0.7009 - val_loss: 1.8322 - val_accuracy: 0.6126\n",
      "Epoch 25/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.8921 - accuracy: 0.6990 - val_loss: 1.8468 - val_accuracy: 0.6156\n",
      "Epoch 26/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.8847 - accuracy: 0.7021 - val_loss: 1.8666 - val_accuracy: 0.6296\n",
      "Epoch 27/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.8743 - accuracy: 0.7061 - val_loss: 1.8258 - val_accuracy: 0.6231\n",
      "Epoch 28/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.8735 - accuracy: 0.7155 - val_loss: 1.8193 - val_accuracy: 0.6500\n",
      "Epoch 29/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.8616 - accuracy: 0.7318 - val_loss: 1.8833 - val_accuracy: 0.6545\n",
      "Epoch 30/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.8619 - accuracy: 0.7306 - val_loss: 1.8885 - val_accuracy: 0.6405\n",
      "Epoch 31/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.8456 - accuracy: 0.7352 - val_loss: 1.8323 - val_accuracy: 0.6590\n",
      "Epoch 32/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.8365 - accuracy: 0.7358 - val_loss: 1.8320 - val_accuracy: 0.6440\n",
      "Epoch 33/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.8331 - accuracy: 0.7370 - val_loss: 1.8607 - val_accuracy: 0.6425\n",
      "Epoch 34/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.8343 - accuracy: 0.7340 - val_loss: 1.8842 - val_accuracy: 0.6440\n",
      "Epoch 35/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.8532 - accuracy: 0.7301 - val_loss: 1.8855 - val_accuracy: 0.6395\n",
      "Epoch 36/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.8328 - accuracy: 0.7424 - val_loss: 1.8510 - val_accuracy: 0.6425\n",
      "Epoch 37/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.8218 - accuracy: 0.7409 - val_loss: 1.8673 - val_accuracy: 0.6430\n",
      "Epoch 38/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.8091 - accuracy: 0.7459 - val_loss: 1.8307 - val_accuracy: 0.6570\n",
      "Epoch 39/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7966 - accuracy: 0.7458 - val_loss: 1.8493 - val_accuracy: 0.6570\n",
      "Epoch 40/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7962 - accuracy: 0.7474 - val_loss: 1.8593 - val_accuracy: 0.6650\n",
      "Epoch 41/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7907 - accuracy: 0.7571 - val_loss: 1.8580 - val_accuracy: 0.6635\n",
      "Epoch 42/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7819 - accuracy: 0.7586 - val_loss: 1.8607 - val_accuracy: 0.6715\n",
      "Epoch 43/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7802 - accuracy: 0.7589 - val_loss: 1.8607 - val_accuracy: 0.6625\n",
      "Epoch 44/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7717 - accuracy: 0.7654 - val_loss: 1.8715 - val_accuracy: 0.6695\n",
      "Epoch 45/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7772 - accuracy: 0.7635 - val_loss: 1.8801 - val_accuracy: 0.6740\n",
      "Epoch 46/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7799 - accuracy: 0.7657 - val_loss: 1.8139 - val_accuracy: 0.6785\n",
      "Epoch 47/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7571 - accuracy: 0.7685 - val_loss: 1.8170 - val_accuracy: 0.6780\n",
      "Epoch 48/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7486 - accuracy: 0.7713 - val_loss: 1.8324 - val_accuracy: 0.6825\n",
      "Epoch 49/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7419 - accuracy: 0.7704 - val_loss: 1.8422 - val_accuracy: 0.6785\n",
      "Epoch 50/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7362 - accuracy: 0.7767 - val_loss: 1.8313 - val_accuracy: 0.6790\n",
      "Epoch 51/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7325 - accuracy: 0.7772 - val_loss: 1.8431 - val_accuracy: 0.6820\n",
      "Epoch 52/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7287 - accuracy: 0.7798 - val_loss: 1.8816 - val_accuracy: 0.6755\n",
      "Epoch 53/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7215 - accuracy: 0.7826 - val_loss: 1.9066 - val_accuracy: 0.6820\n",
      "Epoch 54/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7159 - accuracy: 0.7813 - val_loss: 1.8569 - val_accuracy: 0.6865\n",
      "Epoch 55/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7100 - accuracy: 0.7840 - val_loss: 1.8664 - val_accuracy: 0.6870\n",
      "Epoch 56/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7087 - accuracy: 0.7833 - val_loss: 1.8611 - val_accuracy: 0.6900\n",
      "Epoch 57/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7061 - accuracy: 0.7841 - val_loss: 1.8887 - val_accuracy: 0.6860\n",
      "Epoch 58/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7026 - accuracy: 0.7823 - val_loss: 1.9107 - val_accuracy: 0.6865\n",
      "Epoch 59/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6967 - accuracy: 0.7834 - val_loss: 1.8864 - val_accuracy: 0.6865\n",
      "Epoch 60/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.7017 - accuracy: 0.7848 - val_loss: 1.9313 - val_accuracy: 0.6875\n",
      "Epoch 61/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6955 - accuracy: 0.7861 - val_loss: 1.9150 - val_accuracy: 0.6830\n",
      "Epoch 62/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6874 - accuracy: 0.7891 - val_loss: 1.9713 - val_accuracy: 0.6840\n",
      "Epoch 63/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6959 - accuracy: 0.7855 - val_loss: 1.9573 - val_accuracy: 0.6840\n",
      "Epoch 64/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6869 - accuracy: 0.7896 - val_loss: 1.9541 - val_accuracy: 0.6800\n",
      "Epoch 65/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.6793 - accuracy: 0.7884 - val_loss: 1.9167 - val_accuracy: 0.6920\n",
      "Epoch 66/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6732 - accuracy: 0.7933 - val_loss: 1.9231 - val_accuracy: 0.6870\n",
      "Epoch 67/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6702 - accuracy: 0.7919 - val_loss: 1.9516 - val_accuracy: 0.6890\n",
      "Epoch 68/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6655 - accuracy: 0.7958 - val_loss: 1.8809 - val_accuracy: 0.6865\n",
      "Epoch 69/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6612 - accuracy: 0.7929 - val_loss: 1.9386 - val_accuracy: 0.6875\n",
      "Epoch 70/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6543 - accuracy: 0.7958 - val_loss: 1.9473 - val_accuracy: 0.6970\n",
      "Epoch 71/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6503 - accuracy: 0.8000 - val_loss: 1.9545 - val_accuracy: 0.6960\n",
      "Epoch 72/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6472 - accuracy: 0.7991 - val_loss: 1.9080 - val_accuracy: 0.6980\n",
      "Epoch 73/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6448 - accuracy: 0.8016 - val_loss: 1.9555 - val_accuracy: 0.7104\n",
      "Epoch 74/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6358 - accuracy: 0.8110 - val_loss: 1.9766 - val_accuracy: 0.7094\n",
      "Epoch 75/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6450 - accuracy: 0.8137 - val_loss: 1.9437 - val_accuracy: 0.7144\n",
      "Epoch 76/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6315 - accuracy: 0.8162 - val_loss: 1.9774 - val_accuracy: 0.7039\n",
      "Epoch 77/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6224 - accuracy: 0.8161 - val_loss: 2.0053 - val_accuracy: 0.7159\n",
      "Epoch 78/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6176 - accuracy: 0.8191 - val_loss: 1.9526 - val_accuracy: 0.7094\n",
      "Epoch 79/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6092 - accuracy: 0.8170 - val_loss: 1.9303 - val_accuracy: 0.7189\n",
      "Epoch 80/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6139 - accuracy: 0.8181 - val_loss: 1.9098 - val_accuracy: 0.7204\n",
      "Epoch 81/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6040 - accuracy: 0.8225 - val_loss: 1.8844 - val_accuracy: 0.7244\n",
      "Epoch 82/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.6024 - accuracy: 0.8201 - val_loss: 1.8930 - val_accuracy: 0.7249\n",
      "Epoch 83/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.5927 - accuracy: 0.8261 - val_loss: 1.9105 - val_accuracy: 0.7344\n",
      "Epoch 84/200\n",
      "251/251 [==============================] - 6s 25ms/step - loss: 0.5868 - accuracy: 0.8335 - val_loss: 1.8778 - val_accuracy: 0.7364\n",
      "Epoch 85/200\n",
      "251/251 [==============================] - 6s 25ms/step - loss: 0.5822 - accuracy: 0.8356 - val_loss: 1.9218 - val_accuracy: 0.7424\n",
      "Epoch 86/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.5788 - accuracy: 0.8382 - val_loss: 1.9010 - val_accuracy: 0.7374\n",
      "Epoch 87/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.5709 - accuracy: 0.8403 - val_loss: 1.8828 - val_accuracy: 0.7494\n",
      "Epoch 88/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.5667 - accuracy: 0.8443 - val_loss: 1.8900 - val_accuracy: 0.7414\n",
      "Epoch 89/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.5604 - accuracy: 0.8442 - val_loss: 1.8636 - val_accuracy: 0.7389\n",
      "Epoch 90/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.5564 - accuracy: 0.8459 - val_loss: 1.8488 - val_accuracy: 0.7349\n",
      "Epoch 91/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.5597 - accuracy: 0.8437 - val_loss: 1.8074 - val_accuracy: 0.7499\n",
      "Epoch 92/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.5525 - accuracy: 0.8469 - val_loss: 1.8191 - val_accuracy: 0.7539\n",
      "Epoch 93/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.5440 - accuracy: 0.8501 - val_loss: 1.8256 - val_accuracy: 0.7589\n",
      "Epoch 94/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.5411 - accuracy: 0.8519 - val_loss: 1.8517 - val_accuracy: 0.7569\n",
      "Epoch 95/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.5432 - accuracy: 0.8507 - val_loss: 1.8607 - val_accuracy: 0.7459\n",
      "Epoch 96/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.5350 - accuracy: 0.8527 - val_loss: 1.8295 - val_accuracy: 0.7634\n",
      "Epoch 97/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.5327 - accuracy: 0.8562 - val_loss: 1.8969 - val_accuracy: 0.7599\n",
      "Epoch 98/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.5291 - accuracy: 0.8594 - val_loss: 1.9020 - val_accuracy: 0.7544\n",
      "Epoch 99/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.5285 - accuracy: 0.8556 - val_loss: 1.9133 - val_accuracy: 0.7609\n",
      "Epoch 100/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.5240 - accuracy: 0.8576 - val_loss: 1.8810 - val_accuracy: 0.7639\n",
      "Epoch 101/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.5159 - accuracy: 0.8598 - val_loss: 1.8798 - val_accuracy: 0.7659\n",
      "Epoch 102/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.5123 - accuracy: 0.8605 - val_loss: 1.9107 - val_accuracy: 0.7723\n",
      "Epoch 103/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.5109 - accuracy: 0.8633 - val_loss: 1.8602 - val_accuracy: 0.7664\n",
      "Epoch 104/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.5061 - accuracy: 0.8612 - val_loss: 1.9201 - val_accuracy: 0.7664\n",
      "Epoch 105/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.5035 - accuracy: 0.8600 - val_loss: 1.9087 - val_accuracy: 0.7584\n",
      "Epoch 106/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.5064 - accuracy: 0.8598 - val_loss: 2.0419 - val_accuracy: 0.7514\n",
      "Epoch 107/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.5020 - accuracy: 0.8613 - val_loss: 1.8889 - val_accuracy: 0.7609\n",
      "Epoch 108/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4973 - accuracy: 0.8596 - val_loss: 1.9044 - val_accuracy: 0.7509\n",
      "Epoch 109/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.5054 - accuracy: 0.8568 - val_loss: 1.8992 - val_accuracy: 0.7599\n",
      "Epoch 110/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4967 - accuracy: 0.8598 - val_loss: 1.9322 - val_accuracy: 0.7539\n",
      "Epoch 111/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4948 - accuracy: 0.8598 - val_loss: 1.9372 - val_accuracy: 0.7474\n",
      "Epoch 112/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4899 - accuracy: 0.8588 - val_loss: 1.9091 - val_accuracy: 0.7574\n",
      "Epoch 113/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4827 - accuracy: 0.8600 - val_loss: 1.8995 - val_accuracy: 0.7549\n",
      "Epoch 114/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4775 - accuracy: 0.8598 - val_loss: 1.9152 - val_accuracy: 0.7609\n",
      "Epoch 115/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4781 - accuracy: 0.8608 - val_loss: 1.9172 - val_accuracy: 0.7619\n",
      "Epoch 116/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4717 - accuracy: 0.8607 - val_loss: 1.9131 - val_accuracy: 0.7599\n",
      "Epoch 117/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4654 - accuracy: 0.8613 - val_loss: 1.8979 - val_accuracy: 0.7629\n",
      "Epoch 118/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4597 - accuracy: 0.8730 - val_loss: 1.9094 - val_accuracy: 0.7848\n",
      "Epoch 119/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4511 - accuracy: 0.8896 - val_loss: 1.9197 - val_accuracy: 0.7838\n",
      "Epoch 120/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4454 - accuracy: 0.8909 - val_loss: 1.9302 - val_accuracy: 0.7823\n",
      "Epoch 121/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4398 - accuracy: 0.8910 - val_loss: 1.9183 - val_accuracy: 0.7843\n",
      "Epoch 122/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4363 - accuracy: 0.8919 - val_loss: 1.9728 - val_accuracy: 0.7888\n",
      "Epoch 123/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4351 - accuracy: 0.8920 - val_loss: 1.9450 - val_accuracy: 0.7838\n",
      "Epoch 124/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4272 - accuracy: 0.8916 - val_loss: 1.9433 - val_accuracy: 0.7878\n",
      "Epoch 125/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4237 - accuracy: 0.8921 - val_loss: 1.9561 - val_accuracy: 0.7878\n",
      "Epoch 126/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4228 - accuracy: 0.8929 - val_loss: 1.9528 - val_accuracy: 0.7793\n",
      "Epoch 127/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4194 - accuracy: 0.8928 - val_loss: 1.9240 - val_accuracy: 0.7878\n",
      "Epoch 128/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4109 - accuracy: 0.8920 - val_loss: 1.9127 - val_accuracy: 0.7803\n",
      "Epoch 129/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4138 - accuracy: 0.8898 - val_loss: 1.9328 - val_accuracy: 0.7863\n",
      "Epoch 130/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4135 - accuracy: 0.8915 - val_loss: 2.1060 - val_accuracy: 0.7713\n",
      "Epoch 131/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4096 - accuracy: 0.8909 - val_loss: 1.9845 - val_accuracy: 0.7853\n",
      "Epoch 132/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4010 - accuracy: 0.8921 - val_loss: 1.9197 - val_accuracy: 0.7833\n",
      "Epoch 133/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.4049 - accuracy: 0.8913 - val_loss: 1.9855 - val_accuracy: 0.7808\n",
      "Epoch 134/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3999 - accuracy: 0.8921 - val_loss: 1.9895 - val_accuracy: 0.7823\n",
      "Epoch 135/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3888 - accuracy: 0.8939 - val_loss: 1.9912 - val_accuracy: 0.7753\n",
      "Epoch 136/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3905 - accuracy: 0.8914 - val_loss: 1.9459 - val_accuracy: 0.7833\n",
      "Epoch 137/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3857 - accuracy: 0.8915 - val_loss: 1.9283 - val_accuracy: 0.7858\n",
      "Epoch 138/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3810 - accuracy: 0.8928 - val_loss: 1.9578 - val_accuracy: 0.7868\n",
      "Epoch 139/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3784 - accuracy: 0.8944 - val_loss: 1.9914 - val_accuracy: 0.7918\n",
      "Epoch 140/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3753 - accuracy: 0.8940 - val_loss: 2.0341 - val_accuracy: 0.7938\n",
      "Epoch 141/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3867 - accuracy: 0.8933 - val_loss: 2.0196 - val_accuracy: 0.7878\n",
      "Epoch 142/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3731 - accuracy: 0.8948 - val_loss: 2.0252 - val_accuracy: 0.7943\n",
      "Epoch 143/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3717 - accuracy: 0.8941 - val_loss: 2.0240 - val_accuracy: 0.7928\n",
      "Epoch 144/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3698 - accuracy: 0.8954 - val_loss: 1.9893 - val_accuracy: 0.8003\n",
      "Epoch 145/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3657 - accuracy: 0.8981 - val_loss: 1.9876 - val_accuracy: 0.7968\n",
      "Epoch 146/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.3634 - accuracy: 0.8991 - val_loss: 2.0542 - val_accuracy: 0.7863\n",
      "Epoch 147/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.3796 - accuracy: 0.8955 - val_loss: 1.9590 - val_accuracy: 0.7968\n",
      "Epoch 148/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3713 - accuracy: 0.8975 - val_loss: 1.9904 - val_accuracy: 0.7878\n",
      "Epoch 149/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3709 - accuracy: 0.9002 - val_loss: 2.0034 - val_accuracy: 0.7918\n",
      "Epoch 150/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3628 - accuracy: 0.9004 - val_loss: 1.9460 - val_accuracy: 0.7948\n",
      "Epoch 151/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3646 - accuracy: 0.9022 - val_loss: 1.9602 - val_accuracy: 0.7968\n",
      "Epoch 152/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3552 - accuracy: 0.9029 - val_loss: 1.9754 - val_accuracy: 0.7993\n",
      "Epoch 153/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3562 - accuracy: 0.9032 - val_loss: 1.9365 - val_accuracy: 0.8033\n",
      "Epoch 154/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3502 - accuracy: 0.9072 - val_loss: 1.9566 - val_accuracy: 0.7958\n",
      "Epoch 155/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3457 - accuracy: 0.9060 - val_loss: 1.9587 - val_accuracy: 0.7983\n",
      "Epoch 156/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3429 - accuracy: 0.9076 - val_loss: 1.9858 - val_accuracy: 0.7968\n",
      "Epoch 157/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3435 - accuracy: 0.9059 - val_loss: 2.0092 - val_accuracy: 0.7988\n",
      "Epoch 158/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3390 - accuracy: 0.9069 - val_loss: 2.0250 - val_accuracy: 0.7963\n",
      "Epoch 159/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3381 - accuracy: 0.9081 - val_loss: 1.9741 - val_accuracy: 0.7983\n",
      "Epoch 160/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3393 - accuracy: 0.9075 - val_loss: 2.0167 - val_accuracy: 0.7963\n",
      "Epoch 161/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3515 - accuracy: 0.9075 - val_loss: 2.0353 - val_accuracy: 0.7968\n",
      "Epoch 162/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3352 - accuracy: 0.9072 - val_loss: 1.9959 - val_accuracy: 0.8033\n",
      "Epoch 163/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3307 - accuracy: 0.9092 - val_loss: 2.0308 - val_accuracy: 0.7983\n",
      "Epoch 164/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3280 - accuracy: 0.9104 - val_loss: 1.9988 - val_accuracy: 0.8068\n",
      "Epoch 165/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3266 - accuracy: 0.9100 - val_loss: 2.0046 - val_accuracy: 0.8083\n",
      "Epoch 166/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3259 - accuracy: 0.9092 - val_loss: 2.0196 - val_accuracy: 0.8088\n",
      "Epoch 167/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3227 - accuracy: 0.9109 - val_loss: 1.9845 - val_accuracy: 0.8103\n",
      "Epoch 168/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3281 - accuracy: 0.9106 - val_loss: 1.9422 - val_accuracy: 0.8073\n",
      "Epoch 169/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3228 - accuracy: 0.9100 - val_loss: 1.9529 - val_accuracy: 0.8098\n",
      "Epoch 170/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3188 - accuracy: 0.9197 - val_loss: 1.9529 - val_accuracy: 0.8208\n",
      "Epoch 171/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3259 - accuracy: 0.9215 - val_loss: 1.9911 - val_accuracy: 0.8193\n",
      "Epoch 172/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3222 - accuracy: 0.9237 - val_loss: 1.9412 - val_accuracy: 0.8278\n",
      "Epoch 173/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3189 - accuracy: 0.9215 - val_loss: 2.0295 - val_accuracy: 0.8133\n",
      "Epoch 174/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3132 - accuracy: 0.9225 - val_loss: 2.0014 - val_accuracy: 0.8178\n",
      "Epoch 175/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3226 - accuracy: 0.9200 - val_loss: 2.0099 - val_accuracy: 0.8173\n",
      "Epoch 176/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3116 - accuracy: 0.9223 - val_loss: 2.0143 - val_accuracy: 0.8273\n",
      "Epoch 177/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3068 - accuracy: 0.9238 - val_loss: 1.9815 - val_accuracy: 0.8263\n",
      "Epoch 178/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3050 - accuracy: 0.9261 - val_loss: 1.9721 - val_accuracy: 0.8233\n",
      "Epoch 179/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3043 - accuracy: 0.9238 - val_loss: 1.9299 - val_accuracy: 0.8233\n",
      "Epoch 180/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3001 - accuracy: 0.9257 - val_loss: 1.9113 - val_accuracy: 0.8278\n",
      "Epoch 181/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.2979 - accuracy: 0.9258 - val_loss: 1.9592 - val_accuracy: 0.8293\n",
      "Epoch 182/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.3000 - accuracy: 0.9253 - val_loss: 1.9300 - val_accuracy: 0.8298\n",
      "Epoch 183/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.2959 - accuracy: 0.9252 - val_loss: 1.9315 - val_accuracy: 0.8318\n",
      "Epoch 184/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.2935 - accuracy: 0.9265 - val_loss: 1.9914 - val_accuracy: 0.8258\n",
      "Epoch 185/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.2902 - accuracy: 0.9283 - val_loss: 2.0143 - val_accuracy: 0.8283\n",
      "Epoch 186/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.2892 - accuracy: 0.9267 - val_loss: 2.0244 - val_accuracy: 0.8228\n",
      "Epoch 187/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.2886 - accuracy: 0.9253 - val_loss: 2.0222 - val_accuracy: 0.8293\n",
      "Epoch 188/200\n",
      "251/251 [==============================] - 6s 25ms/step - loss: 0.2895 - accuracy: 0.9270 - val_loss: 2.0112 - val_accuracy: 0.8273\n",
      "Epoch 189/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.2857 - accuracy: 0.9295 - val_loss: 2.0251 - val_accuracy: 0.8208\n",
      "Epoch 190/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.2883 - accuracy: 0.9258 - val_loss: 2.0302 - val_accuracy: 0.8233\n",
      "Epoch 191/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.3104 - accuracy: 0.9255 - val_loss: 2.0038 - val_accuracy: 0.8258\n",
      "Epoch 192/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.3536 - accuracy: 0.9184 - val_loss: 2.0736 - val_accuracy: 0.8178\n",
      "Epoch 193/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.3417 - accuracy: 0.9213 - val_loss: 2.0574 - val_accuracy: 0.8168\n",
      "Epoch 194/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.3238 - accuracy: 0.9202 - val_loss: 2.0807 - val_accuracy: 0.8023\n",
      "Epoch 195/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.3269 - accuracy: 0.9159 - val_loss: 2.0208 - val_accuracy: 0.8138\n",
      "Epoch 196/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.3110 - accuracy: 0.9215 - val_loss: 1.9481 - val_accuracy: 0.8228\n",
      "Epoch 197/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.3010 - accuracy: 0.9248 - val_loss: 2.0507 - val_accuracy: 0.8173\n",
      "Epoch 198/200\n",
      "251/251 [==============================] - 6s 23ms/step - loss: 0.2941 - accuracy: 0.9247 - val_loss: 2.0945 - val_accuracy: 0.8198\n",
      "Epoch 199/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.2930 - accuracy: 0.9262 - val_loss: 2.0175 - val_accuracy: 0.8268\n",
      "Epoch 200/200\n",
      "251/251 [==============================] - 6s 24ms/step - loss: 0.2831 - accuracy: 0.9283 - val_loss: 1.9656 - val_accuracy: 0.8268\n",
      "63/63 [==============================] - 0s 6ms/step - loss: 1.9656 - accuracy: 0.8268\n",
      "Test Loss: 1.9656076431274414\n",
      "Test Accuracy: 0.8267598748207092\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "model.fit(X_train, y_train_encoded, batch_size=32, epochs=200, validation_data=(X_test, y_test_encoded))\n",
    "\n",
    "#정확도 측정\n",
    "loss, GUR_accuracy = model.evaluate(X_test, y_test_encoded)\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", GUR_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2932837",
   "metadata": {
    "id": "d2932837"
   },
   "source": [
    "# 성능 비교 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "863890f4",
   "metadata": {
    "id": "863890f4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "02b625c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "02b625c3",
    "outputId": "5e267b64-275b-44e1-ae89-8eecae548821"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#그래프로 성능 비교 \n",
    "\n",
    "x = ('CNN', 'LSTM','SVM','MLP','GUR')\n",
    "y = [CNN_accuracy, LSTM_accuracy,SVM_accuracy,MLP_accuracy,GUR_accuracy]\n",
    "plt.xlabel(\"model\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "width = 0.5 \n",
    "plt.bar(x, y,width)  # 데이터 가로 나온 함수 barh\n",
    "plt.title('Comparison of classification accuracy by model')\n",
    "for a,b,i in zip(x,y,range(len(x))): # zip 函数\n",
    "    plt.text(a,b+0.01,\"%.2f\"%y[i],ha='center',fontsize=8) # plt.text 함수\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fdeb44",
   "metadata": {
    "id": "69fdeb44"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
