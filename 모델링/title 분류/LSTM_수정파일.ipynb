{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV 파일 읽기\n",
    "df = pd.read_csv(\"./레이블부여된공지사항(수정본).csv\") \n",
    "\n",
    "notice_names = df['title'].tolist()\n",
    "\n",
    "# 전처리 함수 정의\n",
    "def preprocess_text(text):\n",
    "    # 특수문자 제거\n",
    "    text = re.sub(r\"[^\\uAC00-\\uD7A30-9a-zA-Z\\s]\", \"\", text)\n",
    "\n",
    "    # 숫자 정보 제거\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # 형태소 분석\n",
    "    tagger = Okt()\n",
    "    words = tagger.morphs(text)\n",
    "\n",
    "    # 불용어 제거\n",
    "    stop_words = ['필독', '학기', '학년', '도', '년', '제', '회', '월', '학부', '일', '차', '년도', '안내']  # 불용어 리스트\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # 분석된 형태소들을 공백으로 결합하여 문장으로 반환\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "preprocessed_notice_names = [preprocess_text(notice_name) for notice_name in notice_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가장 긴 문장의 길이: 29\n",
      "어휘 사전의 크기: 6093\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 토크나이저 생성\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# 토크나이저가 데이터를 학습\n",
    "tokenizer.fit_on_texts(preprocessed_notice_names)\n",
    "\n",
    "# 데이터를 시퀀스로 변환\n",
    "sequences = tokenizer.texts_to_sequences(preprocessed_notice_names)\n",
    "\n",
    "# 가장 긴 문장의 길이를 확인\n",
    "max_sequence_length = max(len(s) for s in sequences)\n",
    "print('가장 긴 문장의 길이:', max_sequence_length)\n",
    "\n",
    "# 어휘 사전의 크기를 확인 (+1을 하는 이유는 0인덱스를 고려하기 때문입니다.)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('어휘 사전의 크기:', vocab_size)\n",
    "\n",
    "# 모든 문장을 가장 긴 문장의 길이로 패딩 처리\n",
    "padded_X = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 벡터화\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_notice_names)\n",
    "\n",
    "tfidf_model = TfidfVectorizer().fit(preprocessed_notice_names)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 분리: train 데이터 80%, test 데이터 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 타겟 변수 다중 클래스 원-핫 인코딩\n",
    "y_train_encoded = to_categorical(y_train)\n",
    "y_test_encoded = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 모델 생성\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128, input_length=max_sequence_length))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# 출력 뉴런의 개수와 활성화 함수를 클래스 개수에 따라 조정\n",
    "num_classes = len(set(y))  # 클래스 개수 계산\n",
    "if num_classes == 2:\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "else:\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "248/248 [==============================] - 3s 13ms/step - loss: 0.1986 - accuracy: 0.9500 - val_loss: 4.6844 - val_accuracy: 0.6937\n",
      "Epoch 2/30\n",
      "248/248 [==============================] - 3s 13ms/step - loss: 0.1312 - accuracy: 0.9666 - val_loss: 4.5080 - val_accuracy: 0.7089\n",
      "Epoch 3/30\n",
      "248/248 [==============================] - 3s 13ms/step - loss: 0.0710 - accuracy: 0.9830 - val_loss: 4.6554 - val_accuracy: 0.7200\n",
      "Epoch 4/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0372 - accuracy: 0.9927 - val_loss: 4.6261 - val_accuracy: 0.7240\n",
      "Epoch 5/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0264 - accuracy: 0.9946 - val_loss: 4.5972 - val_accuracy: 0.7281\n",
      "Epoch 6/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0221 - accuracy: 0.9961 - val_loss: 4.6451 - val_accuracy: 0.7250\n",
      "Epoch 7/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0293 - accuracy: 0.9933 - val_loss: 4.7832 - val_accuracy: 0.7089\n",
      "Epoch 8/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0535 - accuracy: 0.9860 - val_loss: 4.7120 - val_accuracy: 0.7144\n",
      "Epoch 9/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0694 - accuracy: 0.9840 - val_loss: 4.7438 - val_accuracy: 0.7104\n",
      "Epoch 10/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0929 - accuracy: 0.9774 - val_loss: 4.8996 - val_accuracy: 0.7023\n",
      "Epoch 11/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.1232 - accuracy: 0.9656 - val_loss: 4.7428 - val_accuracy: 0.7089\n",
      "Epoch 12/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0985 - accuracy: 0.9748 - val_loss: 4.6521 - val_accuracy: 0.7260\n",
      "Epoch 13/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0522 - accuracy: 0.9873 - val_loss: 4.5438 - val_accuracy: 0.7392\n",
      "Epoch 14/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0218 - accuracy: 0.9947 - val_loss: 4.6045 - val_accuracy: 0.7452\n",
      "Epoch 15/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0209 - accuracy: 0.9955 - val_loss: 4.5972 - val_accuracy: 0.7371\n",
      "Epoch 16/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0494 - accuracy: 0.9899 - val_loss: 4.8726 - val_accuracy: 0.6958\n",
      "Epoch 17/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0389 - accuracy: 0.9907 - val_loss: 4.6709 - val_accuracy: 0.7341\n",
      "Epoch 18/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0224 - accuracy: 0.9951 - val_loss: 4.6378 - val_accuracy: 0.7402\n",
      "Epoch 19/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0705 - accuracy: 0.9835 - val_loss: 4.8718 - val_accuracy: 0.7064\n",
      "Epoch 20/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0653 - accuracy: 0.9823 - val_loss: 4.5780 - val_accuracy: 0.7225\n",
      "Epoch 21/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0307 - accuracy: 0.9934 - val_loss: 4.7152 - val_accuracy: 0.7366\n",
      "Epoch 22/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0096 - accuracy: 0.9984 - val_loss: 4.6910 - val_accuracy: 0.7417\n",
      "Epoch 23/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0063 - accuracy: 0.9989 - val_loss: 4.6402 - val_accuracy: 0.7422\n",
      "Epoch 24/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0057 - accuracy: 0.9990 - val_loss: 4.6834 - val_accuracy: 0.7427\n",
      "Epoch 25/30\n",
      "248/248 [==============================] - 3s 13ms/step - loss: 0.0062 - accuracy: 0.9986 - val_loss: 4.7262 - val_accuracy: 0.7407\n",
      "Epoch 26/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0659 - accuracy: 0.9865 - val_loss: 4.6775 - val_accuracy: 0.7255\n",
      "Epoch 27/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.0537 - accuracy: 0.9870 - val_loss: 4.7403 - val_accuracy: 0.7250\n",
      "Epoch 28/30\n",
      "248/248 [==============================] - 3s 12ms/step - loss: 0.1025 - accuracy: 0.9735 - val_loss: 4.5876 - val_accuracy: 0.7099\n",
      "Epoch 29/30\n",
      "248/248 [==============================] - 3s 13ms/step - loss: 0.0556 - accuracy: 0.9844 - val_loss: 4.4607 - val_accuracy: 0.7341\n",
      "Epoch 30/30\n",
      "248/248 [==============================] - 3s 13ms/step - loss: 0.0466 - accuracy: 0.9881 - val_loss: 4.3844 - val_accuracy: 0.7371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1edfde77b20>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 훈련\n",
    "model.fit(X_train, y_train_encoded, epochs=30, batch_size=32, validation_data=(X_test, y_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7371342078708375\n"
     ]
    }
   ],
   "source": [
    "y_prob = model.predict(X_test, verbose=0) \n",
    "y_pred = y_prob.argmax(axis=-1)\n",
    "accuracy = np.sum(y_pred.flatten() == y_test) / len(y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step\n",
      "입력 문장: [전공] 2023학년도 1학기 전공신청 및 변경안내\n",
      "예측된 레이블: 282\n"
     ]
    }
   ],
   "source": [
    "# 새로운 입력 데이터 예측\n",
    "new_text = input(\"새로운 문장을 입력하세요: \")\n",
    "preprocessed_new_text = preprocess_text(new_text)\n",
    "sequence = tokenizer.texts_to_sequences([preprocessed_new_text])\n",
    "padded_sequence = pad_sequences(sequence, maxlen=max_sequence_length, padding='post')\n",
    "prediction = model.predict(padded_sequence)\n",
    "predicted_label = label_encoder.inverse_transform([prediction.argmax()])[0]\n",
    "\n",
    "# 예측 결과 출력\n",
    "print(f\"입력 문장: {new_text}\")\n",
    "print(f\"예측된 레이블: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
